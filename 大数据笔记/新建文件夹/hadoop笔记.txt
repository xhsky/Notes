Hadoop的定义(ASF)：
	Hadoop软件库是一个框架，允许在集群中使用简单的编程对大规模数据集进行分布式计算。它
	被设计为可以从单一服务器扩展到数以千计的本地计算和存储的节点，并且Hadoop会在应用层面
	检测和处理错误，而不依靠硬件的高可用性。
	
	Hadoop是一个提供分布式存储和计算的软件框架，它具有无共享、高可用、弹性可扩展的特点，适合处理海量数据
	官网：http://hadoop.apache.org/
	
Hadoop是一个框架
	1.它是由一系列的软件库组成的框架。主要包括Common(提供远程过程调用RPC、序列化机制等)，HDFS(数据存储)，
	  MapReduce(数据计算)
	2.Hadoop为并行和分布式处理提供了一个计算层，与这个计算层紧密联系的是一个高度容错的存储层——Hadoop分布式
     文件系统（Hadoop Distributed File System，HDFS），而且它们都运行在低价、常见和相互兼容的普通硬件上
	3.Hadoop是MapReduce框架的一个开源实现

搜索引擎：摄取和汇总网页信息
爬虫：遍历网络、摄取文档的程序则被


Hadoop生态圈：
	狭义的hadoop仅代表了Common，HDFS和MapReduce模块，而广义的hadoop包含了多个组件

		Hadoop Common：
			是Hadoop体系中最底层的一个模块，为各个子项目提供工具(如系统配置工具，远程调用RPC，序列化机制和日志操作等)
		HDFS(Hadoop Distributed File System，Hadoop分布式文件系统)：
			是Hadoop的基石。HDFS是一个具有高度容错性的文件系统，适合部署在廉价的机器上。HDFS能提高数据访问的吞吐量，
			非常适合大规模数据集上的应用。
		MapReduce：
			是一种编程模型，利用函数式编程的思想，对数据集处理的过程分为Map和Reduce两个阶段。该编程模型适合进行分布
			式计算。Hadoop提供了MapReduce的计算框架，实现了这种编程模式，用户可通过Java，C++，Python、PHP等进行编程
		HBase：
			来源于谷歌的BigTable论文，HBase是一个分布式的、面向列的开源数据库。采用BigTable的数据模型(键/值存储)。
			HBase擅长大规模数据的随机、实时读写访问。
		ZooKeeper：
			是一个分布式的服务框架，基于Fast Paxos算法，解决了分布式系统中一致性的问题。提供配置维护，名字服务，分布式
			同步，组服务等
		Hive：
			最早由Facebook开发使用，是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，提供简单的
			SQL查询功能，并将SQL语句转换为MapReduce作业运行。其优点是学习成本低，对于常见的数据分析需求不必开发专门
			的MapReduce作业，适合大规模数据统计分析。
		Pig：
			与Hive类似，也是对大型数据集进行分析和评估的工具，不过与Hive提供SQL接口不同，它提供了一种高层的、面向领域
			的抽象语言(Pig Latin)，Pig也可将Pig Latin脚本转化为MapReduce作业。与SQL相比，Pig Latin更加灵活，但学习成本
			稍高
		Impala：
			有Cloudera公司开发，可以对存储在HDFS、HBase的数据提供直接查询互动的SQL。Impala使用统一的存储平台、元数据、
			SQL语法、ODBC驱动和用户界面(Hue Beeswax)。Impala提供了一个面向批量或实时查询的统一平台。对中等数据量的查询
			非常迅速，其性能大幅领先于Hive。(Impala并未像Hive一样基于MapReduce框架)
		Mahout：
			是一个机器学习和数据挖掘库，它利用MapReduce编程模型实现了k-means、Native Bayes、Collaborative Filtering等
			机器学习算法，并使其具有良好的可扩展性
		Flume：
			Cloudera提供的一个高可用、高可靠、分布式的海量日志采集、聚合和传输系统。Flume支持在日志系统中定制各类数据发
			送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各类数据接收方(可定制)的能力
		Sqoop(SQL to Hadoop的缩写)：
			主要作用在结构化的数据存储于Hadoop之间进行数据双向交换。即Sqoop可以将关系型数据库(MySQL，Oracle等)导入到
			Hadoop的HDFS、Hive中，也可将HDFS、Hive的数据库导出到关系型数据库。Sqoop的整个导入导出都是有MapReduce计算框
			架实现并行化，非常高效
		
	Hadoop的二次开发公司：
		Cloudera：
			其开发的CDH(Cloudera's Distribution for Hadoop)是生产环境下装机量最大的hadoop发行版。其特点在于稳定，并有许多
			重要的补丁、向后移植和更新
		Hortonworks：
			最著名的是DAG(有向无环图)计算框架Tez。补充了hadoop中有向无环图的计算
	
	Hadoop的发行版本：
		1.Apache Hadoop：
			是Apace软件基金会的顶级项目，原生Hadoop
			官网：http://hadoop.apache.org/
		2.CDH：
			Cloudera公司的产品，基于Apache许可证。在安装时可根据需要集成各个组件，不存在兼容性问题
		3.
	
Hadoop新版本的特性：
	1.Append：
		HDFS Append 支持对文件的追加(HBase的预写日志(WAL))
	2.Security:
		为Hadoop增加了基于Kerberos和Deletion Token的安全机制
	3.Symlink：
		使HDFS支持符号链接
	4.MRv1：
		第一代MapReduce计算框架，通过MapReduce思想，将问题转化为Map和Reduce两个阶段，基础服务由JobTracker
		、TaskTracker进程提供
	5.YARN/MRv2：
		全新的资源管理框架(Yet Another Resource Negotiator)，通过这个组件，在共用底层存储HDFS的情况下，
		计算框架采用可插拔式的配置。在MRv1中的JobTracker的资源管理和作业跟踪功能被拆分，由ResourceManager和
		ApplicationMaster两个组件来完成，增强了扩展性
	6.NameNode Federation：
		NameNode保存了所有元数据，所以其性能制约了整个HDFS集群的扩展。基于此，NameNode Federation将NameNode
		横向扩展，每个NameNode保存一部分数据，彼此间相互隔离，但共享底层的DataNode存储
	7.NameNode HA：
		采用共享存储的方案解决NameNode的高可用问题
	
	
Apache Hadoop的分支：
	Hadoop 1.0：包含三大版本
		0.20.X，0.21.X，0.22.X
		其中，0.20.X最后演化成1.0.X，0.21.X，0.22.X则加入了NameNode HA等新特性
	
	Hadoop 2.0：包含两个版本
		0.23.X，2.X
		它完全不同与Hadoop 1.0，是一套全新的架构，均包含HDFS Federation和YARN两个系统
		相对于0.23.X，2.X增加了NameNode HA和Write-compatibility两个特性
		
Hadoop架构：
	Hadoop主要由两部分组成：分布式文件系统HDFS和分布式计算框架MapReduce
	
	HDFS架构：
		主从模式：一个NameNode和多个DataNode
		NameNode：一个，
		Secondary NameNode：一个，
		DataNode：多个，
		
												---	 客户端
		Secondary NameNode  -----  NameNode	-------  客户端
												---  客户端
										|
										|
						DataNode	DataNode	DataNode
						
	MapReduce架构：
		主从模式：一个JobTracker和多个TaskTracker
		JobTracker：一个，
		TaskTracke：多个，
		                                           ---	客户端
									JobTracker -------  客户端
												   ---	客户端
										|
										|
						TaskTracke 	TaskTracke	TaskTracke
					
	Hadoop架构：
		DataNode和TaskTracker需要配对部署在同一个节点，在生产环境中为性能和稳定性考虑，NameNode和JobTracker
		须分开部署
			
			
			Secondary NameNode  -----   NameNode ------ JobTracker
											|______________|
												   |
							                       |
								DataNode		DataNode		DataNode
							   TaskTracker	   TaskTracker	   TaskTracker
		
Hadoop的三种运行模式：
	单机模式：		安装简单，几乎不用任何配置，仅限于调试用途
	伪分布模式：	在单节点上同时启动5个进程，模拟分布式运行的各个结点	
	完全分布式模式：正常的Hadoop集群，由多个结点构成	
		安装：
			1.配置/etc/hosts文件 
			2.建立专门的运行Hadoop的用户
			3.配置ssh免密码登录，将各个节点的公钥互相添加到对方的主机中(在Hadoop用户下进行)
			4.安装jdk(root)
			5.解压到/opt，并修改配置文件
				# wget http://apache.fayea.com/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz
				hadoop-env.sh:
					export JAVA_HOME=/usr/local/jdk
				core-site.xml:	
					<property>
					  <name>fs.defaultFS</name>
					  <value>hdfs://master:9000</value>
					</property>
				hdfs-site.xml:
					<property>
					  <name>dfs.replication</name>
					  <value>2</value>
					</property>
					<property>
					  <name>dfs.namenode.name.dir</name>
					  <value>file:///opt/hadoop/dfs/name</value>
					</property>
					<property>
					  <name>dfs.datanode.data.dir</name>
					  <value>file:///opt/hadoop/dfs/data</value>
					</property>
				mapred-site.xml:
					<property>
					  <name>mapreduce.jobtracker.address</name>
					  <value>master:9001</value>
					</property>
				masters:
					master
				slaves:
					slave1
					slave2
			6.配置好后用scp -r发往各个节点并在主节点上配置环境变量
				export HADOOP_HOME=/opt/hadoop
				export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
			7.在Master上格式化分布式文件系统
				# bin/hadoop namenode -format
			8.启动Hadoop		
				# bin/start-all.sh
			9.检测守护进程启动情况
				Master：jdk目录下/bin/jps  （3）
				Slave：jdk目录下/bin/jps	（2）
Hadoop的配置文件：
	在 ./etc/hadoop/目录下
	
	hadoop-env.sh				hadoop环境变量配置文件
	core-site.xml				hadoop核心配置项，如namenode的IP和Port
	hdfs-site.xml				HDFS进程的配置项，包括NameNode，SecondaryNameNode，DataNode等
	mapred-site.xml				MapReduce进程的配置项，如JobTracker的IP和Port
	masters						运行SecondaryNameNode的主机
	slaves						运行DataNode和TaskTracker的主机列表(每行一个)
	 
	hadoop-metrics.properties	控制metrics在hadoop上如何发布的属性
	log4j.properties			系统日志文件，NameNode审计日志，TaskTracker子进程的任务日志的属性
	
	mapred-env.sh
	capacity-scheduler.xml
	configuration.xsl
	container-executor.cfg
	hadoop-metrics2.properties
	hadoop-metrics.properties
	hadoop-policy.xml
	httpfs-env.sh
	httpfs-log4j.properties
	httpfs-signature.secret
	httpfs-site.xml
	kms-acls.xml
	kms-env.sh
	kms-log4j.properties
	kms-site.xml
	mapred-queues.xml.template
	mapred-site.xml.template
	ssl-client.xml.example
	ssl-server.xml.example
	yarn-env.cmd
	yarn-env.sh
	yarn-site.xml

Hadoop客户端：
	说明：该客户端在集群之外，接管所有与计算存储无关的任务。
	安装：
		可将集群内任意一台机器的安装文件，不做任何修改，发送到用来做hadoop客户端
		的节点的相同目录下即可完成
		
		注：在集群的slaves文件中并未写入hadoop客户端的ip，所以不会再此机器上启动任何进程
	
Hadoop性能调优：
	
	硬件： 
		主机：
			主节点：
				1.Hadoop为主从架构，故两者的硬件选择上分为不同的两类。主节点的可用性方面要好于从节点
				2.NameNode启动时会将所有元数据加载入内存。故hdfs上的文件数量受限于NameNode的内存大小
				3.SecondaryNameNode的配置原则上与NameNode相同，当NameNode宕机时，可能会让其替补
				4.JobTracker保存近100个运行在集群上的作业的元数据信息，故在考虑JobTracker的内存时，
				  需考虑集群的使用场景
			从节点：
				计算：
					JobTracker会根据每台机器的任务槽(slot)和使用情况为其分配任务。一般，一个slot需要的
					内存在2-4G，而CPU资源的分配以一个槽占用一个虚拟CPU为准。故：
						任务槽数：		CPU数*单个CPU核数*单个CPU的超线程数
						内存大小：		任务槽数*(2,4)G
						最多处理的数据：单个任务处理的数据量*任务槽数
				存储：
					要考虑块的副本数。若副本数为3，则1T的文件实际存储为3T，在平均到每个节点。同时要为
					临时数据保留20%-30%的空间
		网络:
			hadoop作业是数据密集型而非计算密集型。瓶颈往往在I/O上面。故需注意交换机，规划网络
			
		注：集群规模规划往往根据存储空间来考虑整个集群的大小

	操作系统：
		1.避免使用Swap分区：
			swap的使用通过vm.swappiness参数进行控制，值域为0-100，值越高，则说明操作系统内核更积极地
			去将应用程序的数据交换到swap。而将hadoop进程的数据交换到磁盘的行为有可能导致操作超时，所以
			应该将值设为0
		2.调整内存分配策略：
			操作系统根据vm.overcommit_memory来决定对进程内存的分配策略：
				0：系统有足够内存则分配，反之申请内存失败
				1：允许分配所有的物理内存，不管当前的内存状态
				2：允许分配超过所有物理内存和swap总和的内存，并通过vm/overcommit_ratio设置超过的比例，
				   50代表超过物理内存50%
			建议将其设置为2，并调整vm.overcommit_ratio的值
		3.调整net.core.somaxconn值
			该参数表示套接字(socket)监听的backlog上限。backlog是socket的监听队列(当一个请求尚未被处理
			或建立时，会进入backlog。而套接字服务可一次性处理backlog中的所有请求)。当服务器处理请求较
			慢，以至于监听队列被填满后，新来的请求被拒绝。
			
			在core-site.xml中，ipc.server.listen.queue.size参数控制了backlog的监听长度，Linux的
			net.core.somaxconn的参数设定了backlog的监听长度，其值默认均为128
			
			建议将两个参数值设定为大于等于32768
		4.增大文件描述符的上限
		5.选择合适的文件系统
			不同的文件系统性能不同。
			当文件系统被格式化后，需要禁用文件的访问时间(每次读操作的时候都会进行一次写操作)。HDFS
			不支持修改操作，且获取访问时间无意义。所以在挂载数据分区时，需禁止文件系统的访问时间
		6.关闭THP(Transparent Huge Pages)
			Huge Pages是一个大小为2M-1G的内存叶，而THP是一个管理和使用Huge Pages自动化的抽象层。但、
			在运行hadoop作业时，THP会引起CPU占用率偏高，需要关闭
	JVM：
		主要调整JVM FLAGES和JVM GC，调整后执行效率大概有4%的提升
	Hadoop：
		hadoop级别的调优主要通过调整hadoop的参数来完成
		1.hdfs.site.xml
			dfs.blocksize：
				hadoop文件块大小，默认128M。经过实际测试发现，128M的性能表现要好于64M，256M和384M
			dfs.datanode.handler.count：
				NameNode同时和DataNode通信的线程数，默认是10，可将其增大为40
			dfs.datanode.max.transfer.threads:
				DataNode的最大连接数，超过配置数则拒绝连接，可改为65536
			dfs.datanode.balance.bandwidthPerSec
				执行start-balncer.sh的带宽，默认1M，可将其增大到20M
		2.core-site.xml	
			io.file.buffer.size：
				hadoop的缓冲区大小。用于hadoop读取和写入hdfs的文件，和map的中间结果。默认4k，可增加
				至20k
		3.mapred.site.xml
			mapreduce.tasktracker.map.tasks.maximum：
				map任务的槽数，即同时运行map任务的最大数量
			mapreduce.tasktracker.reduce.tasks.maximum：
				reduce任务的槽数，即同时运行reduce任务的最大数量
				
				注：一般此参数明显小于map任务的数量。且每个节点的槽数加起来不能超过虚拟CPU的个数
		1.可配置独立于集群之外的hadoop客户端
	
	
		注：hadoo的调优主要遵循3个原则：
			1.增大作业的并行度，如增大Map作业的数量
			2.保证任务执行时有足够的资源
			3.满足前两者的前提下，尽可能地为Shuffle阶段提供资源
			
			这三点对很多分布式框架同样有用，如spark
		
		
当集群节点足够多时，故障将成为一种常态而非异常现象
	
	


MapReduce：
	Map-Reduce的思想：分而治之
		1.Mapper负责“分”，将复杂的任务分解为若干个简单的任务
			1.这些简单的任务的数据或计算规模相对于原任务要大大缩小
			2.就近计算，即会被分配到存放了所需数据的结点进行计算
			3.这些任务可以并行计算，彼此几乎没有依赖关系
		2.Reduce对map阶段的结果进行汇总，其数目由mapred-site.xml配置文件里的项目mapred.reduce.tasks决定，缺省值为1
		3.Shuffler
			1.在Mapper和Reduce之间的一个步骤(可以没有)
			2.可以把mapper的输出按照魔种key值重新切分和组合成n份，把key值符合某种范围的输出到特定的Reduce中
			3.简化Reduce的过程
			
	调度机制
		·缺省为先入先出作业队列调度
		·支持公平调度器
		·支持容量调度器		
	
	性能调优
		1.Reduce的数量
		2.输入：大文件优于小文件(多个小文件合并成大文件，Hadoop适合处理大文件)
		3.减少网络传输：压缩map的输出
		4.优化每个节点运行的任务数
			mapred.tasktracker.map.tasks.maximum		缺省值为2
			mapred.tasktracker.reduce.tasks.maximum		缺省值为2
	
	任务执行优化
		推测式执行：
			1.若jobtracker发现有拖后腿的任务，会在启动一个相同的备份任务，然后哪个先执行完就会kill掉另外一个。					因此在监控网页上经常会看到正常执行完的作业有被					kill掉的任务
			2.推测式执行缺省打开，但若是代码问题，并不能解决问题，且使集群更慢。通过在mapred-site.xml文件中设
			  置mapred.map.tasks.speculative.execution和mapred.reduce.tasks.speculative.execution可为map任务或
			  reduce任务开启或是关闭推测式执行
		- 重用JVM：	
			可以省掉启动新的JVM消耗的时间。在mapred-site.xml中设置mapred.job.reuse.jvm.num.tasks设置单个JVM运行
			的最大任务数（-1表示无限制）
		- 忽略模式：	
			任务在读取数据失败2次后，会把数据位置告诉jobtracker，后者重新启动该任务并在遇到所记录的坏数据时直
			接跳过(缺省关闭，用SkipBadRecord方法打开)
			
	错误处理机制：
		硬件故障
			·硬件故障是指jobtracker或tasktracker故障
			·jobtracker是单点，若发生故障目前还无法处理，它通过心跳信号了解tasktracker是否发生故障或负载过于严重
			·jobtracker将从任务节点列表中移除发生故障的tasktracker
			·若故障节点在执行map任务且尚未完成，jobtracker会要求其它节点重新执行此map任务
			·若故障接待你在执行reduce任务且尚未完成，jobtracker会要求其它节点继续执行此任务
		任务失败
			·由于代码缺陷或是进程崩溃
			·JVM自动退出，向tasktracker父进程发送错误信息，错误信息也会写入日志
			·tasktracker监听程序会发现进程退出或是长时间没有更新信息回复，则将任务标记为失败。
			·标记失败任务后，任务计数器减去1以便接受新任务，并通过心跳信号告诉jobtracker任务失败的信息
			·jobtracker获悉任务失败后，将该任务重新放入调度队列，重新分配出去在执行
			·若一个任务失败4次(可设置)，将不会再被执行，同时作业也宣布失败
	使用Hadoop API开发MapReduce程序
		确定目标 ----> 开发软件(Eclipse) ----> 测试结果
	
Hive：
	
	Hive是Hadoop的一个客户端，可以安装到hadoop集群的任意一个节点
	安装：
		1.配置JDK
		2.安装元数据库
		3.修改Hive配置文件
	
	
	
	
	