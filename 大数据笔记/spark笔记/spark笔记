简介
	时间, 作者, 开发语言, 定义
    用于大规模数据分析的统一引擎, 一种多语言引擎, 用于执行数据工程、数据科学和机器学习. 
	官网: https://spark.apache.org/
	版本
	协议
适用性(优缺)
  1.简单, 快速, 可扩展
  2.批处理/流式数据
  3.SQL分析
  4.大规模的数据科学
  5.机器学习
架构
	模块
	安装
    1.安装jdk1.8
    2.安装scala
      $ wget -c https://downloads.lightbend.com/scala/2.12.15/scala-2.12.15.tgz
      $ tar -xf scala-2.12.15.tgz -C /dream
      $ cd /dream/;  ln -sv scala-2.12.15 scala
    3.配置变量
      $ vim /etc/profile
        export JAVA_HOME=/dream/jdk
        export SCALA_HOME=/dream/scala
        export PATH=$JAVA_HOME/bin:$SCALA_HOME/bin:$PATH
	结构
		目录结构
			安装目录
        bin:
        conf:
          fairscheduler.xml
          metrics.properties
          spark-env.sh                          # 环境变量配置
            JAVA_HOME=/path
            PYSPARK_PYTHON=/path                # 用于驱动程序和workers程序中的PySpark的Python二进制可执行文件(如果可用, 默认为python3,否则为python
            PYSPARK_DRIVER_PYTHON=/path         # 用于驱动程序中的PySpark的Python二进制可执行文件, 默认PYSPARK_PYTHON
            SPARKR_DRIVER_R=/path               # 用于SparkR shell的R二进制可执行文件
            SPARK_LOCAL_IP=                     # 要绑定的机器的IP地址
            SPARK_PUBLIC_DNS=                   # Spark程序将向其他机器通告的主机名
            HADOOP_CONF_DIR=                    # 使用hdfs相关时的hadoop配置文件目录(一般包含hdfs-site.xml和core-site.xml)
            > 以下用于standalone配置
            SPARK_MASTER_HOST=                  # 将master绑定到特定的主机名或IP地址
            SPARK_MASTER_PORT=7077              # master端口, 默认7077 
            SPARK_MASTER_WEBUI_PORT=8080        # master web ui的端口8080
            SPARK_MASTER_OPTS=                  # 仅适用于"-Dx=y"形式的master的配置属性
                                                > spark.deploy.retainedApplications=200           # 在ui中显示已完成的application最大数量, 较旧的删除. 默认200
                                                > spark.deploy.retainedDrivers=200                # 在ui中显示已完成的driver最大数量. 较旧的删除. 默认200
                                                > spark.deploy.spreadOut=true                     # 在standalone中是否将application分散到各节点上(或整合到尽可能少的节点上). 对于hdfs上的局部性数据, 分散更好. 但对于计算密集型整合更好. 默认为true
                                                > spark.deploy.defaultCores=                      # 
                                                > spark.deploy.maxExecutorRetries=10              # 
                                                > spark.worker.timeout=60                         # 在standalone中master未收到worker心跳的超时秒数(认为worker丢失)
                                                > spark.worker.resource.{resourceName}.amount=    # 在worker上使用的特定资源的数量. 默认为none
                                                > spark.worker.resource.{resourceName}.discoveryScript=           # 资源发现脚本的路径, 用于在worker启动时查找特定资源. 并且脚本的输出应该像ResourceInformation类一样格式化. 默认none
                                                > spark.worker.resourcesFile=                     # 资源文件的路径, 用于worker启动时查找各种资源
            SPARK_LOCAL_DIRS=                   # 在Spark中用于"临时"空间的目录, 包括map输出文件和存储在磁盘上的RDD

            SPARK_WORKER_CORES=                 # 允许Spark应用在服务器上使用的cpu核心总数, 默认所有可用CPU
            SPARK_WORKER_MEMORY=                # 允许Spark应用在机器上使用的内存总量. 默认总内存-1G. 
            SPARK_WORKER_PORT=                  # spark worker的端口(默认随机)
            SPARK_WORKER_WEBUI_PORT=            # worker ui的端口(默认8081)
            SPARK_WORKER_DIR=                   # 运行application的目录, 包含logs和临时空间(默认SPARK_HOME/work)
            SPARK_WORKER_OPTS=                  # 仅适用于"-Dx=y"形式的worker的配置属性
                                                > spark.worker.cleanup.enabled=false               # 启用worker/application目录的定期清理(只影响standalone模式, yarn工作模式不同). 默认false. 只清理已停止/超时application的文件或子目录
                                                > spark.worker.cleanup.interval=1800               # 控制worker清理本地旧应用程序工作目录的时间间隔. 默认1800秒
                                                > spark.worker.cleanup.appDataTtl=604800           # 在每个worker上保留应用程序工作目录的秒数. 默认7天
                                                > spark.shuffle.service.db.enabled=true            # 
                                                > spark.storage.cleanupFilesAfterExecutorExit=true # 在executor退出后启用清理工作目录的非shuffle文件(eg: temp, shuffle blocks, cached RDD/broadcast blocks, spill file等). 当前只对standalone模式生效
                                                > spark.worker.ui.compressedLogFileLengthCacheSize=100             #  

            SPARK_DAEMON_MEMORY=1g              # 分配给master和worker自身的内存, 默认1g
            SPARK_DAEMON_JAVA_OPTS=             # Spark master和worker进程本身的JVM选项, 格式为"-Dx=y"
            SPARK_DAEMON_CLASSPATH=             # Spark master和worker进程本身的类路径, 默认none
          workers
          log4j.properties                      # 日志配置
          spark-defaults.conf                   # spark属性, 由bin/spark-submit读取
            spark.app.name  none                # application的名称
            spark.driver.cores  1               # 用于driver process的vcpu数. 仅用于cluster mode
            spark.driver.maxResultSize  1g      # 每个Spark操作(例如收集)的所有分区的序列化结果的总大小限制. 默认1g
            spark.driver.memory 1g              # driver进程的内存大小. 默认1g
            spark.driver.memoryOverhead         # 在cluster mode下每个driver程序的non-heap内存. 默认driver.memory * 0.1 最小为384M
            spark.driver.resource.{resourceName}.amount 0         # 要在driver程序上使用的特定资源类型的数量
            spark.driver.resource.{resourceName}.discoveryScript  # driver程序运行以发现特定资源类型的脚本
            spark.driver.resource.{resourceName}.vendor           # 用于驱动程序的资源的供应商. 此选项目前仅在 Kubernetes 上受支持
            spark.resources.discoveryPlugin 
            spark.executor.memory 1g            # 每个executor程序进程使用的内存量
            spark.executor.pyspark.memory       # 每个executor程序中分配给PySpark的内存量. 若未设置, Spark将不会限制Python的内存使用
            spark.executor.memoryOverhead       # 每个executor程序进程要分配的额外内存量. 默认executor.memory * 0.1 最小为384M
            spark.executor.resource.{resourceName}.amount 
            spark.executor.resource.{resourceName}.discoveryScript  
            spark.executor.resource.{resourceName}.vendor

            spark.extraListeners                #   
            spark.local.dir /tmp                # Spark中用于"scratch"空间的目录, 包括map输出文件和存储在磁盘上的RDD. 可为逗号分割的多个目录
            spark.logConf false                 # 
            spark.master                        # 要连接的集群管理器
                                                > local: 使用一个工作线程在本地运行 Spark
                                                > local[K]: 使用K个工作线程在本地运行Spark
                                                > local[K,F]: 使用K个工作线程和F个maxFailures在本地运行Spark
                                                > local[*]: 使用与node上的vcpu一样多的工作线程在本地运行 Spark
                                                > local[*,F]: 
                                                > local-cluster[N,C,M]: local-cluster模式仅用于单元测试.它在单个JVM中模拟分布式集群, 具有N个工作器, 每个工作器使用C个vcpu和M Mib内存
                                                > spark://HOST:PORT : 连接到standalone cluster master. 默认为 7077
                                                > spark://HOST1:PORT1,HOST2:PORT2  : 连接到使用zk的standalone cluster. 端口默认7077
                                                > yarn: 连接到yarn集群(使用client mode或cluster mode). yarn配置必须在HADOOP_CONF_DIR或YARN_CONF_DIR中
                                                > k8s://HOST:PORT: 使用client或cluster模式连接到Kubernetes集群. HOST和PORT指的是Kubernetes API服务器. 它默认使用TLS连接. 为了强制它使用不安全的连接,可使用k8s://http://HOST:PORT
            spark.submit.deployMode             # 
            spark.log.callerContext             # o
            spark.driver.supervise  false       # 若为true, 则当driver以非0状态退出时自动重新启动. 仅对standalone和mesos模式有效
            spark.driver.log.dfsDir             # o
            spark.driver.log.persistToDfs.enabled false         # 
            spark.driver.log.layout 
            spark.driver.log.allowErasureCoding 
        data:
        jars:
		进程/端口
      master:
        java:
          7077: Master通信端口
          8080: Standalone模式Master Web端口
      worker:
        java:
          8081: worker的Web端口
          34818
      SparkSubmit:
        java:
          4040: Spark context Web UI
          35555
		编程接口
		管理软件
	命令
		服务器
		客户端
	日志
	优化
	安全
	集群
		
具体服务相关
	概念:
    Application: 基于Spark构建的用户程序. 由集群上的Driver program和executors组成
    Driver program: 运行main()函数并创建SparkContext的进程
    Executor: 在worker node上启动的进程, 它运行tasks并在内存/磁盘中保存数据
    Worker node: 运行application code的节点
    Task: 发送给executer的work unit
    Cluster manager: 用于获取集群资源的外部服务(standalone manager, Mesos, YARN, Kubernetes)
    Deploy mode:  区分driver process运行的位置. 在cluster mode下, framework在cluster内部启动driver. 在client mode下, submitter在集群外部启动driver
    Job: 由多个task组成的并行计算, 这些job响应spark操作(eg: save, collect)而产生. 
    Stage: 每个job都会分成更小的task集合. 称为stage. 这些stage相互依赖
    Application jar: 包含用户application的jar

  流程:
    1.spark使用指定配置从集群管理器中请求具有相应资源的容器
    2.获取容器后, spark会在该容器中启动一个executor(发现容器拥有的资源及其每个资源关联的地址)
    3.executor向driver注册并报告其可用的资源
    4.spark scheduler将task调度到executor上并分配报告资源中的资源地址
    5.在driver上, 用户可使用分配的资源地址进行相关的处理

  模块:

    Structured Streaming: 使用关系查询处理结构化数据流(使用Datasets和DataFrames, 比DStreams更新的API)
    Spark Streaming: 使用DStreams(旧API)处理数据流
    MLlib: 应用机器学习算法
    GraphX: 处理图
    
	内部命令
